{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b87ed0-3df6-4b0b-8ea7-3869bcbc2d37",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503d9fae-c502-4d23-bbf7-6da631b5ad9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dddex/lib/python3.8/site-packages/pandas/__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _dependency \u001b[38;5;129;01min\u001b[39;00m _hard_dependencies:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dependency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:\n\u001b[1;32m     13\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/dddex/lib/python3.8/site-packages/numpy/__init__.py:143\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# NOTE: to be revisited following future namespace cleanup.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# See gh-14454 and gh-15672 for discussion.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/dddex/lib/python3.8/site-packages/numpy/lib/__init__.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistograms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolynomial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraysetops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnpyio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:934\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from dddex.levelSetKDEx_univariate import LevelSetKDEx, LevelSetKDEx_NN\n",
    "from dddex.wSAA import RandomForestWSAA, SampleAverageApproximation\n",
    "from dddex.crossValidation import QuantileCrossValidation, QuantileCrossValidationLSx, groupedTimeSeriesSplit\n",
    "from dddex.utils import generateFinalOutput\n",
    "# from generalFuncs.runModels import tunePredictSave, getCostsNV\n",
    "# import generalFuncs\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "import re\n",
    "import ipdb\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355fe8c7-c013-4b86-a2bd-666e59f33adb",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991226ba-6a0f-42ed-8bc7-9c799b11e1ea",
   "metadata": {},
   "source": [
    "## Main Function - Tune Predict Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f32ba-84e1-4460-ae31-d94d1272c6d0",
   "metadata": {},
   "source": [
    "NOTE: decisionArtifact and costArtifact must have been logged already before calling `tunePredictSave`. This is required because of the current\n",
    "way `updateArtifact` is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f975c8-bd88-42c8-8406-78a5e2b48836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunePredictSave(quantileEstimator,\n",
    "                    combinedCV,\n",
    "                    model,\n",
    "                    estimatorName):\n",
    "    \n",
    "    runName = model + \"_\" + estimatorName\n",
    "    runName = runName + \"_\" + \"combinedCV\" if combinedCV else runName\n",
    "    runName_SLTuning = runName + \"_SLTuning\"\n",
    "    \n",
    "    config = {'isModellingRun': True,\n",
    "              'model': model,\n",
    "              'estimator': estimatorName,\n",
    "              'weightsByDistance': weightsByDistance,\n",
    "              'combinedCV': combinedCV,\n",
    "              'SLTuning': False,\n",
    "              'kFolds': kFolds,\n",
    "              'nIter': nIter}\n",
    "    \n",
    "    run = wandb.init(project = project, name = runName, job_type = \"modelling\", config = config)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if combinedCV:\n",
    "        \n",
    "        if estimatorName == 'LGBM':\n",
    "            paramGridEstimator = paramGridLGBM\n",
    "        else:\n",
    "            paramGridEstimator = paramGridRF\n",
    "            \n",
    "        LSxCV = QuantileCrossValidationLSx(estimatorLSx = quantileEstimator, \n",
    "                                           cvFolds = cvFolds,\n",
    "                                           parameterGridLSx = binSizeGrid,\n",
    "                                           parameterGridEstimator = paramGridEstimator,\n",
    "                                           randomSearchEstimator = True,\n",
    "                                           nIterEstimator = nIter,\n",
    "                                           probs = probs,\n",
    "                                           refitPerProb = True,\n",
    "                                           n_jobs = 1)\n",
    "        \n",
    "        paramsEstimatorForOutput = {}\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if model == 'WSAA':\n",
    "            paramGrid = paramGridRF\n",
    "            randomSearch = True\n",
    "            paramsEstimatorForOutput = {}\n",
    "        else:\n",
    "            paramGrid = binSizeGrid\n",
    "            randomSearch = False\n",
    "            \n",
    "            if estimatorName == 'LGBM':\n",
    "                paramsEstimatorForOutput = paramsLGBM\n",
    "            else:\n",
    "                paramsEstimatorForOutput = paramsRF\n",
    "            \n",
    "        LSxCV = QuantileCrossValidation(estimator = quantileEstimator, \n",
    "                                        cvFolds = cvFolds,\n",
    "                                        parameterGrid = paramGrid,\n",
    "                                        randomSearch = randomSearch,\n",
    "                                        nIter = nIter,\n",
    "                                        probs = probs,\n",
    "                                        refitPerProb = True,\n",
    "                                        n_jobs = 1)\n",
    "\n",
    "    LSxCV.fit(X = XTrain, \n",
    "              y = yTrain)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Results without Service-Level-Tuning\n",
    "    quantilesDf = LSxCV.bestEstimator.predict(X = XTest, \n",
    "                                              probs = probs, \n",
    "                                              outputAsDf = True, \n",
    "                                              scalingList = scalingList)\n",
    "\n",
    "    quantilesDf.columns = colnamesQuantile\n",
    "\n",
    "    resDf = generateFinalOutput(dataOriginal = data, \n",
    "                                dataDecisions = quantilesDf, \n",
    "                                targetVariable = 'demand', \n",
    "                                mergeOn = None, \n",
    "                                variablesToAdd = ['dayIndex', 'date', 'scalingValue'], \n",
    "                                scaleBy = 'scalingValue', \n",
    "                                includeTraining = False, \n",
    "                                sortBy = ['id', 'dayIndex'],\n",
    "                                longFormat = True,\n",
    "                                **paramsEstimatorForOutput,\n",
    "                                **LSxCV.bestParams)\n",
    "    \n",
    "    costsPerID, costsPerSL = getCostsNV(resDf = resDf,\n",
    "                                        costsPerID_SAA = costsPerID_SAA)\n",
    "    \n",
    "    wandb.log(costsPerSL)\n",
    "    wandb.finish()\n",
    "    \n",
    "    updateArtifact(name = 'decisionData',\n",
    "                   dataToAdd = resDf,\n",
    "                   fileNameToAdd = runName)\n",
    "    \n",
    "    updateArtifact(name = 'costData',\n",
    "                   dataToAdd = costsPerID,\n",
    "                   fileNameToAdd = runName) \n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Results with Service-Level-Tuning\n",
    "    config['SLTuning'] = True\n",
    "    run = wandb.init(project = project, name = runName_SLTuning, job_type = \"modelling\", config = config)\n",
    "    \n",
    "    resList = list()\n",
    "\n",
    "    for prob in probs:   \n",
    "\n",
    "        quantilesDf = LSxCV.bestEstimator_perProb[prob].predict(X = XTest, \n",
    "                                                                probs = prob, \n",
    "                                                                outputAsDf = True, \n",
    "                                                                scalingList = scalingList)\n",
    "\n",
    "        quantilesDf.columns = [\"quantile_\" + str(int(prob * 1000))]\n",
    "\n",
    "        resDf_prob = generateFinalOutput(dataOriginal = data,\n",
    "                                         dataDecisions = quantilesDf,\n",
    "                                         targetVariable = 'demand',\n",
    "                                         mergeOn = None,\n",
    "                                         variablesToAdd = ['dayIndex', 'date', 'scalingValue'],\n",
    "                                         scaleBy = 'scalingValue',\n",
    "                                         includeTraining = False,\n",
    "                                         longFormat = True,\n",
    "                                         **paramsEstimatorForOutput,\n",
    "                                         **LSxCV.bestParams_perProb[prob])\n",
    "\n",
    "        resList.append(resDf_prob)\n",
    "\n",
    "    resDf_SLTuning = pd.concat(resList, axis = 0)\n",
    "    \n",
    "    costsPerID_SLTuning, costsPerSL_SLTuning = getCostsNV(resDf = resDf_SLTuning,\n",
    "                                                          costsPerID_SAA = costsPerID_SAA)\n",
    "    \n",
    "    wandb.log(costsPerSL_SLTuning)\n",
    "    wandb.finish()\n",
    "    \n",
    "    updateArtifact(name = 'decisionData',\n",
    "                   dataToAdd = resDf_SLTuning,\n",
    "                   fileNameToAdd = runName_SLTuning)\n",
    "    \n",
    "    updateArtifact(name = 'costData',\n",
    "                   dataToAdd = costsPerID_SLTuning,\n",
    "                   fileNameToAdd = runName_SLTuning)  \n",
    "    \n",
    "    # return resDf, costsPerID, costsPerSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580fafb-76d3-49f9-a818-b6cfbae35840",
   "metadata": {},
   "source": [
    "## Update Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d6a0-733f-44ef-986b-499797df1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateArtifact(name,\n",
    "                   dataToAdd,\n",
    "                   fileNameToAdd,\n",
    "                   alias = 'latest',\n",
    "                   ):\n",
    "\n",
    "    run = wandb.init(project = project, name = 'updateArtifacts', job_type = 'updateArtifact', config = {'isModellingRun': False})\n",
    "    \n",
    "    artifactOld = run.use_artifact(name + ':' + alias)\n",
    "    artifactFolder = artifactOld.download()\n",
    "\n",
    "    fileNames = os.listdir(artifactFolder)\n",
    "\n",
    "    artifactNew = wandb.Artifact(name = name,\n",
    "                                 type = artifactOld.type,\n",
    "                                 description = artifactOld.description,\n",
    "                                 metadata = artifactOld.metadata)\n",
    "\n",
    "    for fileOld in fileNames:\n",
    "        path = os.path.join(artifactFolder, fileOld)\n",
    "\n",
    "        with open(path, 'rb') as file:\n",
    "            dataOld = pd.read_pickle(file)\n",
    "\n",
    "        with artifactNew.new_file(fileOld, mode = \"wb\") as file:\n",
    "            dataOld.to_pickle(file)\n",
    "\n",
    "    with artifactNew.new_file(fileNameToAdd + '.pkl', mode = \"wb\") as file:\n",
    "            dataToAdd.to_pickle(file)\n",
    "\n",
    "    wandb.log_artifact(artifactNew)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    shutil.rmtree(artifactFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f77320-9358-402a-a2a2-547d9ab287a0",
   "metadata": {},
   "source": [
    "## Load Current Data To Continue after Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110a617-94df-400d-8719-73ff9628752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataToContinue():\n",
    "    \n",
    "    global costsPerID_SAA\n",
    "    global estimatorLGBM\n",
    "    global estimatorRF\n",
    "    global paramsLGBM\n",
    "    global paramsRF\n",
    "    global data\n",
    "    global XTrain\n",
    "    global yTrain\n",
    "    global XTest\n",
    "    global yTest\n",
    "    global scalingList\n",
    "    global nIter\n",
    "    global kFolds\n",
    "    global probs\n",
    "    global colnamesQuantile\n",
    "    global cvFolds\n",
    "    global binSizeGrid\n",
    "    global paramGridLGBM\n",
    "    global paramGridRF\n",
    "    global paramGridRF_LGBM\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    run = wandb.init(project = project, name = \"loadDataToContinue\", job_type = \"loadData\", config = {'isModellingRun': False})\n",
    "    \n",
    "    #---\n",
    "\n",
    "    artifactPredictorData = run.use_artifact('predictorData:latest')\n",
    "    predictorDataFolder = artifactPredictorData.download()\n",
    "\n",
    "    pathParamsLGBM = os.path.join(predictorDataFolder, \"paramsLGBM.pkl\")\n",
    "    with open(pathParamsLGBM, 'rb') as file:\n",
    "        paramsLGBM = pickle.load(file)\n",
    "        \n",
    "    pathParamsRF = os.path.join(predictorDataFolder, \"paramsRF.pkl\")\n",
    "    with open(pathParamsRF, 'rb') as file:\n",
    "        paramsRF = pickle.load(file)\n",
    "\n",
    "    pathEstimatorLGBM = os.path.join(predictorDataFolder, \"LGBM.pkl\")\n",
    "    with open(pathEstimatorLGBM, 'rb') as file:\n",
    "        estimatorLGBM = pickle.load(file)\n",
    "\n",
    "    pathEstimatorRF = os.path.join(predictorDataFolder, \"RF.pkl\")\n",
    "    with open(pathEstimatorRF, 'rb') as file:\n",
    "        estimatorRF = pickle.load(file)\n",
    "        \n",
    "    shutil.rmtree(predictorDataFolder)\n",
    "\n",
    "    #---\n",
    "\n",
    "    artifactCosts = run.use_artifact('costData:latest')\n",
    "    costsDataFolder = artifactCosts.download()\n",
    "\n",
    "    pathCostsSAA = os.path.join(costsDataFolder, \"SAA.pkl\")\n",
    "    costsPerID_SAA = pd.read_pickle(pathCostsSAA)\n",
    "    \n",
    "    shutil.rmtree(costsDataFolder)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    artifactModelData = run.use_artifact('modelData:latest')\n",
    "    modelDataFolder = artifactModelData.download()\n",
    "    \n",
    "    pathData = os.path.join(modelDataFolder, \"data.pkl\")\n",
    "    data = np.load(pathData, allow_pickle = True)\n",
    "    scalingList = data['scalingValue'][data['label'] == 'test'].to_list()\n",
    "    \n",
    "    filesToLoad = [\"XTrain\", \"yTrain\", \"XTest\", \"yTest\"]\n",
    "    \n",
    "    fileDictModelData = {}\n",
    "    for fileName in filesToLoad:\n",
    "        pathFile = os.path.join(modelDataFolder, fileName + \".pkl\")\n",
    "        fileDictModelData[fileName] = np.load(pathFile, allow_pickle = True)\n",
    "    \n",
    "    XTrain = fileDictModelData[\"XTrain\"]\n",
    "    yTrain = fileDictModelData[\"yTrain\"]\n",
    "    XTest = fileDictModelData[\"XTest\"]\n",
    "    yTest = fileDictModelData[\"yTest\"]\n",
    "    \n",
    "    shutil.rmtree(modelDataFolder)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    artifactCVData = run.use_artifact('cvData:latest')\n",
    "    cvDataFolder = artifactCVData.download()\n",
    "    \n",
    "    filesToLoad = [\"cvHyperParameters\", \"probs\", \"colnamesQuantile\", \"cvFolds\", \"binSizeGrid\", \"paramGridLGBM\", \"paramGridRF\", \"paramGridRF_LGBM\"]\n",
    "    \n",
    "    fileDictCV = {}\n",
    "    for fileName in filesToLoad:\n",
    "        pathFile = os.path.join(cvDataFolder, fileName + \".pkl\")\n",
    "        \n",
    "        with open(pathFile, 'rb') as file:\n",
    "            fileDictCV[fileName] = pickle.load(file)        \n",
    "    \n",
    "    nIter = fileDictCV[\"cvHyperParameters\"][\"nIter\"]\n",
    "    kFolds = fileDictCV[\"cvHyperParameters\"][\"kFolds\"]\n",
    "    probs = fileDictCV[\"probs\"]\n",
    "    colnamesQuantile = fileDictCV[\"colnamesQuantile\"]\n",
    "    cvFolds = fileDictCV[\"cvFolds\"]\n",
    "    binSizeGrid = fileDictCV[\"binSizeGrid\"]\n",
    "    paramGridLGBM = fileDictCV[\"paramGridLGBM\"]\n",
    "    paramGridRF = fileDictCV[\"paramGridRF\"]\n",
    "    paramGridRF_LGBM = fileDictCV[\"paramGridRF_LGBM\"]\n",
    "    \n",
    "    shutil.rmtree(cvDataFolder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c0ab8-155a-493b-9f38-356a8555830b",
   "metadata": {},
   "source": [
    "## Get Newsvendor Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf18d2f-922f-4bcc-8c41-19fed794d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCostsNV(resDf,\n",
    "               costsPerID_SAA = None):\n",
    "    \n",
    "    serviceLevels = np.array([int(re.findall('[0-9]+', decisionType)[0]) / 1000 for decisionType in resDf['decisionType']])\n",
    "    # serviceLevels = resDf['decisionType'] * 10\n",
    "    \n",
    "    errors = resDf['actuals'] - resDf['decisions']\n",
    "    resDf['costs'] = np.where(errors >= 0, errors * serviceLevels, np.abs(errors) * (1 - serviceLevels))\n",
    "\n",
    "    costsPerID = resDf.groupby(['id', 'decisionType'], sort = False)['costs'].sum()\n",
    "    \n",
    "    if not costsPerID_SAA is None:\n",
    "        costsPerID = costsPerID / costsPerID_SAA\n",
    "    \n",
    "    costsPerSL = costsPerID.reset_index().groupby(['decisionType'], sort = False)['costs'].mean()\n",
    "    costsPerSL = costsPerSL.to_dict()\n",
    "    \n",
    "    return costsPerID, costsPerSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb50a52-93d6-4afa-9e05-0960fe3f5bb3",
   "metadata": {},
   "source": [
    "# Setup Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a4312-99e1-45f9-a420-7c8f1f9a8cd7",
   "metadata": {},
   "source": [
    "## Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36054dc-6a4f-4ab3-a167-887d9f7bedc6",
   "metadata": {},
   "source": [
    "### Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b505a4-2d61-4ffe-b458-6b163b98cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'run_SID.ipynb'\n",
    "os.environ['WANDB_SILENT'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca53c1-717d-4e78-a2ce-f4fbade32204",
   "metadata": {},
   "source": [
    "### Project Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de56008-f57d-4f4f-a31f-c6b2025b353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'modellingSID'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5987a5-a13b-4586-8f7a-41d0a8df5ed2",
   "metadata": {},
   "source": [
    "### Load and Save Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97b447-c4b4-48aa-b4c9-21b6d93cfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': False}\n",
    "\n",
    "wandb.init(project = project, name = 'loadModelData', job_type = 'loadData', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9c86d-137e-4d85-a4dc-211987b98589",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/kagu/SID/data/dataSID.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# ids = data.id.unique()[0:2]\n",
    "# filtering = [ID in ids for ID in data.id]\n",
    "# data = data[filtering]\n",
    "\n",
    "X = np.array(data.drop(['demand', 'date', 'id', 'label'], axis = 1))\n",
    "Y = np.array(data['demand'])\n",
    "\n",
    "indicesTrain = data['label'] == 'train'\n",
    "indicesTest = data['label'] == 'test'\n",
    "\n",
    "XTrain = X[indicesTrain]\n",
    "yTrain = Y[indicesTrain]\n",
    "\n",
    "XTest = X[indicesTest]\n",
    "yTest = Y[indicesTest]\n",
    "\n",
    "dataTrain = data[indicesTrain]\n",
    "dataTest = data[indicesTest]\n",
    "\n",
    "scalingList = dataTest['scalingValue'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dafe8e-acf8-40e2-a666-648b899825ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "testLength = sum(data[data.id == data.id[0]].label == 'test')\n",
    "numberOfIDs = len(data['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1c8af-da86-46c9-9412-006758154ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelData = wandb.Artifact(name = \"modelData\", \n",
    "                           type = \"dataset\",\n",
    "                           description = \"Train/Test data used for modelling\",\n",
    "                           metadata = {\"testLength\": testLength,\n",
    "                                       \"numberOfIDs\": numberOfIDs})\n",
    "\n",
    "with modelData.new_file(\"data.pkl\", mode = \"wb\") as file:\n",
    "    data.to_pickle(file)\n",
    "\n",
    "datasets = [XTrain, yTrain, XTest, yTest]\n",
    "names = [\"XTrain\", \"yTrain\", \"XTest\", \"yTest\"]\n",
    "\n",
    "for name, dataset in zip(names, datasets):\n",
    "    with modelData.new_file(name + \".pkl\", mode = \"wb\") as file:\n",
    "        np.save(file, dataset)      \n",
    "    \n",
    "wandb.log_artifact(modelData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2bb6e-fe7e-4d2b-8d5c-25d28d3ad33b",
   "metadata": {},
   "source": [
    "### Set and Save Cross Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd26361-6c6c-4141-a40c-c1822c7db0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': False}\n",
    "\n",
    "wandb.init(project = project, name = 'crossValidationData', job_type = 'setCVData', config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119dfba-3561-4fd7-b5a6-2a08a0bb3e55",
   "metadata": {},
   "source": [
    "#### Probs of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca8353-9582-4627-bbef-395587effba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.concatenate([np.array([0.005, 0.025, 0.165, 0.835, 0.975, 0.995]), np.arange(1, 100, 1) / 100])\n",
    "probs = np.sort(probs)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765213bd-4cd0-484a-adf5-2841bf9694c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "probsPermille = np.around(probs * 1000, decimals = 0)\n",
    "probsName = [str(int(i)) for i in iter(probsPermille)]\n",
    "\n",
    "colnamesQuantile = ['quantile_{}'.format(i) for i in iter(probsName)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0782c6-3070-4cc6-af61-7f0a3456ef2a",
   "metadata": {},
   "source": [
    "#### CV Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250f56a-e2d5-4029-8e9a-40d8d6ace3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 120\n",
    "kFolds = 4\n",
    "\n",
    "cvHyperParameters = {\"nIter\": nIter,\n",
    "                     \"kFolds\": kFolds}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25150f15-445c-4a96-bceb-55c78e51fab5",
   "metadata": {},
   "source": [
    "#### Cross Validation Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8e2a5-34ae-4809-81d6-f0fe23390267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Split\n",
    "cvFolds = groupedTimeSeriesSplit(data = dataTrain, \n",
    "                                 kFolds = kFolds, \n",
    "                                 testLength = testLength, \n",
    "                                 groupFeature = 'id', \n",
    "                                 timeFeature = 'dayIndex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05368c-a0b6-4426-b260-1fd509bd3799",
   "metadata": {},
   "source": [
    "#### Param Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f614bff-7d2a-48b1-bd26-67ecacf58d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGridLGBM = {'num_leaves': [10, 25, 40, 60, 80, 100, 150, 200, 250, 300],\n",
    "                 'max_depth': [-1, 3, 4, 5, 6, 7],\n",
    "                 'min_child_samples': [10, 30, 50, 75, 100, 150, 250, 400, 600, 800, 1000, 1500, 2000, 3000, 5000, 10000],\n",
    "                 'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "                 'n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "                 'subsample': [0.05, 0.1, 0.2, 0.3, 0.5, 0.75, 1],\n",
    "                 'colsample_bytree': [0.05, 0.1, 0.2, 0.3, 0.5, 0.75, 1]}\n",
    "\n",
    "paramGridRF_LGBM = {'max_depth': [-1, 2, 3, 4, 5, 6, 7],\n",
    "                    'min_child_samples': [10, 30, 50, 75, 100, 150, 250, 400, 600, 800, 1000, 1500, 2000, 3000, 5000, 10000],\n",
    "                    'n_estimators': [100, 150, 200, 250, 300, 400, 500, 600],\n",
    "                    'subsample': [0.1, 0.3, 0.5, 0.75],\n",
    "                    'subsample_freq' : [1],\n",
    "                    'colsample_bytree': [0.1, 0.3, 0.5, 0.75, 1]}\n",
    "\n",
    "paramGridRF = [{'max_depth': [2, 3, 4, 5, 6, 7, 8, 10],\n",
    "               'min_samples_leaf': [10, 30, 50, 75, 100, 150, 250, 400, 600, 800, 1000, 1500, 2000, 3000, 5000, 10000],\n",
    "               'max_features': [0.1, 0.3, 0.5, 0.75, 1],\n",
    "               'n_estimators': [100, 150, 200, 250, 300, 400, 500, 600],\n",
    "               'max_samples': [0.1, 0.3, 0.5, 0.75, 1],\n",
    "               'bootstrap' : [True]},\n",
    "               {'max_depth': [2, 3, 4, 5, 6, 7, 8, 10],\n",
    "               'min_samples_leaf': [10, 30, 50, 75, 100, 150, 250, 400, 600, 800, 1000, 1500, 2000, 3000, 5000, 10000],\n",
    "               'max_features': [0.1, 0.3, 0.5, 0.75, 1],\n",
    "               'n_estimators': [100, 150, 200, 250, 300, 400, 500, 600],\n",
    "               'max_samples': [None],\n",
    "               'bootstrap' : [False]}]\n",
    "               \n",
    "\n",
    "#---\n",
    "\n",
    "binSizeGrid = {'binSize': [200, 300, 400, 500, 750, 1000, \n",
    "                           1500, 2000, 2500, 3000, \n",
    "                           4000, 5000, 6000]\n",
    "                           # 8000, 10000]}\n",
    "\n",
    "# binSizeGrid = {'binSize': [200, 750, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011c303-fe7c-4e85-ab94-c2cc3b1de876",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvData = wandb.Artifact(name = \"cvData\", \n",
    "                        type = \"model\",\n",
    "                        description = \"Cross-Validation Data - Contains Param Grids and Folds\")\n",
    "\n",
    "grids = [probs, colnamesQuantile, cvHyperParameters, cvFolds, paramGridLGBM, paramGridRF_LGBM, paramGridRF, binSizeGrid]\n",
    "names = [\"probs\", \"colnamesQuantile\", \"cvHyperParameters\", \"cvFolds\", \"paramGridLGBM\", \"paramGridRF_LGBM\", \"paramGridRF\", \"binSizeGrid\"]\n",
    "\n",
    "for name, grid in zip(names, grids):\n",
    "    with cvData.new_file(name + \".pkl\", mode = \"wb\") as file:\n",
    "        pickle.dump(grid, file)\n",
    "        \n",
    "wandb.log_artifact(cvData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399cb4f-8a3f-4159-809c-a20bb3228401",
   "metadata": {},
   "source": [
    "### Additional Artifacts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4e70e-448f-41db-8791-e7babe81a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionArtifact = wandb.Artifact(name = \"decisionData\", \n",
    "                                  type = \"decisions\",\n",
    "                                  description = \"Test decisions for all models\",\n",
    "                                  metadata = {\"testLength\": testLength,\n",
    "                                              \"numberOfIDs\": numberOfIDs})\n",
    "\n",
    "costArtifact = wandb.Artifact(name = \"costData\", \n",
    "                              type = \"costs\",\n",
    "                              description = \"Costs per id for all models\",\n",
    "                              metadata = {\"testLength\": testLength,\n",
    "                                          \"numberOfIDs\": numberOfIDs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca3e9f-6e70-42bf-a886-c9adb2c1fa6b",
   "metadata": {},
   "source": [
    "# Tune Point Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8ef08-5084-42de-aaa4-b0346cd68ec4",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac743e-298f-4cbd-a4b3-9ea35e3c98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': False,\n",
    "          'estimator': 'LGBM',\n",
    "          'kFolds': kFolds,\n",
    "          'nIter': nIter}\n",
    "\n",
    "wandb.init(project = project, name = \"tuneLGBM\", job_type = \"tuneModels\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67bbe5-c2f3-4d52-8005-7188859bc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor\n",
    "estimator = LGBMRegressor(n_jobs = 1)\n",
    "\n",
    "# Cross Validation\n",
    "paramSearch = RandomizedSearchCV(estimator = estimator,\n",
    "                                 cv = cvFolds,\n",
    "                                 n_iter = nIter,\n",
    "                                 param_distributions = paramGridLGBM,\n",
    "                                 scoring = 'neg_mean_squared_error',\n",
    "                                 refit = True,\n",
    "                                 return_train_score = True,\n",
    "                                 n_jobs = 10,\n",
    "                                 random_state = 4444,\n",
    "                                 verbose = 0)\n",
    "\n",
    "paramSearch.fit(X = XTrain,\n",
    "                y = yTrain)\n",
    "\n",
    "estimatorLGBM = paramSearch.best_estimator_\n",
    "paramsLGBM = paramSearch.best_params_\n",
    "        \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b30c89-c1a1-4cc8-b2e8-127af07e0d96",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d55c31-da10-49bd-80d1-97575dd7103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': False,\n",
    "          'estimator': 'RF',\n",
    "          'kFolds': kFolds,\n",
    "          'nIter': nIter}\n",
    "\n",
    "wandb.init(project = project, name = \"tuneRF\", job_type = \"tuneModels\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d5be9-7bcc-4075-8296-5533d371f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor\n",
    "estimator = RandomForestRegressor(n_jobs = 1)\n",
    "\n",
    "# Cross Validation\n",
    "paramSearch = RandomizedSearchCV(estimator = estimator,\n",
    "                                 cv = cvFolds,\n",
    "                                 n_iter = nIter,\n",
    "                                 param_distributions = paramGridRF,\n",
    "                                 scoring = 'neg_mean_squared_error',\n",
    "                                 refit = True,\n",
    "                                 return_train_score = True,\n",
    "                                 n_jobs = 10,\n",
    "                                 random_state = 4444,\n",
    "                                 verbose = 0)\n",
    "\n",
    "paramSearch.fit(X = XTrain,\n",
    "                y = yTrain)\n",
    "\n",
    "estimatorRF = paramSearch.best_estimator_\n",
    "paramsRF = paramSearch.best_params_\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51148d62-babf-404b-a504-55834c95374b",
   "metadata": {},
   "source": [
    "## Save Models + Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152f5d8-fa9b-482f-addf-d4c984b5edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': False,\n",
    "          'kFolds': kFolds,\n",
    "          'nIter': nIter}\n",
    "\n",
    "wandb.init(project = project, name = 'saveTunedPredictors', job_type = 'saveData', config = config)\n",
    "\n",
    "predictorData = wandb.Artifact(name = \"predictorData\", \n",
    "                               type = \"model\",\n",
    "                               description = \"Tuned and fitted point predictor models and their parameters.\")\n",
    "\n",
    "with predictorData.new_file(\"LGBM.pkl\", mode = \"wb\") as file:\n",
    "    pickle.dump(estimatorLGBM, file)\n",
    "    \n",
    "with predictorData.new_file(\"RF.pkl\", mode = \"wb\") as file:\n",
    "    pickle.dump(estimatorRF, file)\n",
    "    \n",
    "with predictorData.new_file(\"paramsLGBM.pkl\", mode = \"wb\") as file:\n",
    "    pickle.dump(paramsLGBM, file)\n",
    "        \n",
    "with predictorData.new_file(\"paramsRF.pkl\", mode = \"wb\") as file:\n",
    "    pickle.dump(paramsRF, file)\n",
    "    \n",
    "wandb.log_artifact(predictorData)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea98a5-2a5c-4e94-8087-963c7039c30a",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb89e96-aaba-48bb-b497-af5db380d5dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SAA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456df93-0d23-4123-abdd-c20350bfa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': True,\n",
    "          'model': 'SAA',\n",
    "          'kFolds': kFolds,\n",
    "          'nIter': nIter}\n",
    "\n",
    "wandb.init(project = project, name = \"SAA\", job_type = \"modelling\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6df00-1972-422d-b3fb-111edb09a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = data['id'].unique()\n",
    "\n",
    "dataResultsList = list()\n",
    "\n",
    "for ID in IDs:\n",
    "    data_id = data[data['id'] == ID]\n",
    "    \n",
    "    y_id = np.array(data_id['demand'])\n",
    "    X_id = np.array(data_id.drop(['demand', 'id', 'label'], axis = 1))\n",
    "    \n",
    "    indicesTrain_id = data_id['label'] == 'train'\n",
    "    indicesTest_id = data_id['label'] == 'test'\n",
    "    \n",
    "    yTrain_id = y_id[indicesTrain_id]\n",
    "    XTrain_id = X_id[indicesTrain_id]\n",
    "\n",
    "    yTest_id = y_id[indicesTest_id]\n",
    "    XTest_id = X_id[indicesTest_id]\n",
    "    \n",
    "    scalingList_id = data_id[indicesTest_id]['scalingValue'].tolist()\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    SAA_id = SampleAverageApproximation()\n",
    "    SAA_id.fit(y = yTrain_id)\n",
    "    \n",
    "    quantilesDfOneOb = SAA_id.predict(X = None, probs = probs, outputAsDf = True, scalingList = scalingList_id)\n",
    "    quantilesDf_id = pd.concat([quantilesDfOneOb] * XTest_id.shape[0], axis = 0).reset_index(drop = True)\n",
    "    quantilesDf_id.columns = colnamesQuantile\n",
    "    \n",
    "    resDf_id = generateFinalOutput(dataOriginal = data_id, \n",
    "                                   dataDecisions = quantilesDf_id, \n",
    "                                   targetVariable = 'demand', \n",
    "                                   mergeOn = None, \n",
    "                                   variablesToAdd = ['dayIndex', 'date', 'scalingValue'], \n",
    "                                   scaleBy = 'scalingValue', \n",
    "                                   includeTraining = False, \n",
    "                                   sortBy = ['id', 'dayIndex'],\n",
    "                                   longFormat = True)\n",
    "     \n",
    "    dataResultsList.append(resDf_id)\n",
    "    \n",
    "#---\n",
    "\n",
    "resDf = pd.concat(dataResultsList, axis = 0).reset_index(drop = True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824533f-7bc7-468b-8fe8-7aa851cd74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "costsPerID_SAA, costsPerSL_SAA = getCostsNV(resDf = resDf)\n",
    "\n",
    "# We need to reuse costsPerSL_SAA later on to compute the other cost ratios\n",
    "costsPerSL_SAA = pd.Series(costsPerSL_SAA)\n",
    "\n",
    "costsRatioPerSL_SAA = {decisionType: 1 for decisionType in costsPerSL_SAA.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438ff78-822f-4d14-96ae-9c5d2c419b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log({'costsPerSL': wandb.Table(data = pd.DataFrame([costsRatioPerSL_SAA]))})\n",
    "wandb.log(costsRatioPerSL_SAA)\n",
    "\n",
    "with decisionArtifact.new_file(\"SAA.pkl\", mode = \"wb\") as file:\n",
    "    resDf.to_pickle(file)\n",
    "    \n",
    "with costArtifact.new_file(\"SAA.pkl\", mode = \"wb\") as file:\n",
    "    costsPerID_SAA.to_pickle(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd0171-7e69-4fe0-bd8d-63e6307231e5",
   "metadata": {},
   "source": [
    "The next step is necessary to be able to resume computation at any given point without rerunning SAA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cf607-fc5f-4814-ada5-4b867d010cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costsSAA = wandb.Artifact(name = \"costsSAA\", \n",
    "#                           type = \"dataset\",\n",
    "#                           description = \"Costs per ID of SAA (fitted per ID)\")\n",
    "\n",
    "# with costsSAA.new_file(\"costsPerID_SAA.pkl\", mode = \"wb\") as file:\n",
    "#     costsPerID_SAA.to_pickle(file)\n",
    "\n",
    "# wandb.log_artifact(costsSAA)\n",
    "    \n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f6d7a-abcf-4f48-a5af-df002c29df21",
   "metadata": {},
   "source": [
    "## SAA Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888018f-2c5d-4831-af71-7da2fccd5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': True,\n",
    "          'model': 'SAA_global',\n",
    "          'kFolds': kFolds,\n",
    "          'nIter': nIter}\n",
    "\n",
    "wandb.init(project = project, name = \"SAA_global\", job_type = \"modelling\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88fd7a-1bb3-4aab-8fd6-4ec38435e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAA_global = SampleAverageApproximation()\n",
    "SAA_global.fit(y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e0903-7c4b-4c6c-b3ca-f64a76b600b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantilesDfOneOb = SAA_global.predict(X = None, probs = probs, outputAsDf = True, scalingList = None)\n",
    "\n",
    "quantilesDf = pd.concat([quantilesDfOneOb] * XTest.shape[0], axis = 0).reset_index(drop = True)\n",
    "quantilesDf.columns = colnamesQuantile\n",
    "\n",
    "quantilesDf = (quantilesDf.T * np.array(scalingList)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23175ac0-b5ff-4d51-b9bc-c1d297c7a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resDf = generateFinalOutput(dataOriginal = data, \n",
    "                            dataDecisions = quantilesDf, \n",
    "                            targetVariable = 'demand', \n",
    "                            mergeOn = None, \n",
    "                            variablesToAdd = ['dayIndex', 'date', 'scalingValue'], \n",
    "                            scaleBy = 'scalingValue', \n",
    "                            includeTraining = False, \n",
    "                            sortBy = ['id', 'dayIndex'],\n",
    "                            longFormat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f1c5f-1df6-4a63-ab7e-ecb878e059a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "costsPerID, costsPerSL = getCostsNV(resDf = resDf,\n",
    "                                    costsPerID_SAA = costsPerID_SAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a5da8-89b5-4e8b-831e-cd6dedf5ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log(costsPerSL)\n",
    "\n",
    "with decisionArtifact.new_file(\"SAA_global.pkl\", mode = \"wb\") as file:\n",
    "    resDf.to_pickle(file)\n",
    "    \n",
    "with costArtifact.new_file(\"SAA_global.pkl\", mode = \"wb\") as file:\n",
    "    costsPerID.to_pickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31801030-cdc6-43f0-a381-5a881ac2c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log_artifact(decisionArtifact)\n",
    "wandb.log_artifact(costArtifact)\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee27a9-4b17-4fe2-b4bf-87b47500d2fa",
   "metadata": {},
   "source": [
    "## LSx Standard - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08073b-9822-4494-aac2-090506124860",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorName = 'LGBM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5671047-039a-4206-b3fe-50a7dccb168d",
   "metadata": {},
   "source": [
    "### weightsByDistance = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176d927-cfea-4f3b-8ca7-c0c490ada41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsByDistance = False\n",
    "model = 'LSx'\n",
    "\n",
    "LSKDEx = LevelSetKDEx(estimator = estimatorLGBM,\n",
    "                      weightsByDistance = weightsByDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf82fd-1433-4d93-bd6a-31e9eefcc67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tunePredictSave(quantileEstimator = LSKDEx,\n",
    "                combinedCV = False,\n",
    "                model = model,\n",
    "                estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84125d9-2a12-4707-bd69-3dd7ba4a6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunePredictSave(quantileEstimator = LSKDEx,\n",
    "#                 combinedCV = True,\n",
    "#                 model = model,\n",
    "#                 estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa4a46-f82b-4dff-9971-94c2f57e9f54",
   "metadata": {},
   "source": [
    "### weightsByDistance = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ced3a-4634-4e87-a20e-9d702ab72ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsByDistance = True\n",
    "model = 'LSx_distWeights'\n",
    "\n",
    "LSKDEx = LevelSetKDEx(estimator = estimatorLGBM,\n",
    "                      weightsByDistance = weightsByDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fbff0-147c-4943-9b84-bba6cdfdd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunePredictSave(quantileEstimator = LSKDEx,\n",
    "                combinedCV = False,\n",
    "                model = model,\n",
    "                estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840dcf2-87d8-47b6-8be8-1f8857999130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunePredictSave(quantileEstimator = LSKDEx,\n",
    "#                 combinedCV = True,\n",
    "#                 model = model,\n",
    "#                 estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a54f4-65bd-41a7-b29e-0793bc06d948",
   "metadata": {},
   "source": [
    "## LSx Standard - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94663af6-7959-44b0-862b-a2aca7d817e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorName = 'RF'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8522d05-e3ae-45c3-8ec9-34d0d9d3087e",
   "metadata": {},
   "source": [
    "### weightsByDistance = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33ef51-89b0-46e8-a352-a79fa5bcd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsByDistance = False\n",
    "model = 'LSx'\n",
    "\n",
    "LSKDEx = LevelSetKDEx(estimator = estimatorRF,\n",
    "                      weightsByDistance = weightsByDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a57988-8a2b-4ca1-ba86-e53f7a12a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunePredictSave(quantileEstimator = LSKDEx,\n",
    "                combinedCV = False,\n",
    "                model = model,\n",
    "                estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e40fa-8a25-494d-b6c6-1dc1562f7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunePredictSave(quantileEstimator = LSKDEx,\n",
    "#                 combinedCV = True,\n",
    "#                 model = model,\n",
    "#                 estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32629fd-6967-404a-96ae-b36a5e35ec0b",
   "metadata": {},
   "source": [
    "### weightsByDistance = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0c067-e40f-49ca-a134-46ac7af815b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsByDistance = True\n",
    "model = 'LSx_distWeights'\n",
    "\n",
    "LSKDEx = LevelSetKDEx(estimator = estimatorRF,\n",
    "                      weightsByDistance = weightsByDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339c7af-c0e8-4c58-a4e3-1b9bebaa6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunePredictSave(quantileEstimator = LSKDEx,\n",
    "                combinedCV = False,\n",
    "                model = model,\n",
    "                estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459b399-a805-4979-a8d8-d8d9d56b1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunePredictSave(quantileEstimator = LSKDEx,\n",
    "#                 combinedCV = True,\n",
    "#                 model = model,\n",
    "#                 estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96650e8-677c-47ee-bbf2-9beca9562bdf",
   "metadata": {},
   "source": [
    "## LSx NN - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d55e1a-9b44-4b19-a6dd-8f1ea5b43309",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorName = 'LGBM'\n",
    "model = 'LSx_NN'\n",
    "weightsByDistance = True\n",
    "\n",
    "LSKDEx = LevelSetKDEx_NN(estimator = estimatorLGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d38ad-2523-41f7-ab47-bb666d4b5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunePredictSave(quantileEstimator = LSKDEx,\n",
    "                combinedCV = False,\n",
    "                model = model,\n",
    "                estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82778548-b901-48eb-a5be-225ca869e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunePredictSave(quantileEstimator = LSKDEx,\n",
    "#                 combinedCV = True,\n",
    "#                 model = model,\n",
    "#                 estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee37e3f-531c-42f6-8873-7c8513ba1cfb",
   "metadata": {},
   "source": [
    "## RF WSAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436eeb1-7d11-43b0-b302-99fac7423857",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorName = 'RF'\n",
    "model = 'WSAA'\n",
    "weightsByDistance = False\n",
    "\n",
    "RFWSAA = RandomForestWSAA()\n",
    "\n",
    "#---\n",
    "\n",
    "tunePredictSave(quantileEstimator = RFWSAA,\n",
    "                combinedCV = False,\n",
    "                model = model,\n",
    "                estimatorName = estimatorName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0450f-b2a0-4cee-ad6a-b79791a28dc9",
   "metadata": {},
   "source": [
    "## RF WSAA - No Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dda2da-b326-4560-bd72-45dc9904a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'isModellingRun': True,\n",
    "          'model': \"WSAA\",\n",
    "          'estimator': \"RF\",\n",
    "          'weightsByDistance': False,\n",
    "          'combinedCV': False,\n",
    "          'kFolds': kFolds,\n",
    "          'nIter': nIter}\n",
    "    \n",
    "wandb.init(project = project, name = \"RF_standard\", job_type = \"modelling\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752a683-0422-42df-a5f0-7da89ea41a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFWSAA = RandomForestWSAA(**paramsRF)\n",
    "\n",
    "RFWSAA.fit(X = XTrain, y = yTrain)\n",
    "\n",
    "quantilesDf = RFWSAA.predict(X = XTest, \n",
    "                             probs = probs, \n",
    "                             outputAsDf = True, \n",
    "                             scalingList = scalingList)\n",
    "\n",
    "quantilesDf.columns = colnamesQuantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fa6cf-4cb7-4df4-817b-99d86278432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resDf = generateFinalOutput(dataOriginal = data, \n",
    "                            dataDecisions = quantilesDf, \n",
    "                            targetVariable = 'demand', \n",
    "                            mergeOn = None, \n",
    "                            variablesToAdd = ['dayIndex', 'date', 'scalingValue'], \n",
    "                            scaleBy = 'scalingValue', \n",
    "                            includeTraining = False, \n",
    "                            sortBy = ['id', 'dayIndex'],\n",
    "                            longFormat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5848bc-3083-463c-a914-4b5530deb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "costsPerID, costsPerSL = getCostsNV(resDf = resDf,\n",
    "                                    costsPerID_SAA = costsPerID_SAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4799e-299a-4667-8b4d-0f4ce7fe40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log(costsPerSL)\n",
    "\n",
    "updateArtifact(name = 'decisionData',\n",
    "               dataToAdd = resDf,\n",
    "               fileNameToAdd = 'RF_standard')\n",
    "    \n",
    "updateArtifact(name = 'costData',\n",
    "               dataToAdd = costsPerID,\n",
    "               fileNameToAdd = 'RF_stanard') \n",
    "    \n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
