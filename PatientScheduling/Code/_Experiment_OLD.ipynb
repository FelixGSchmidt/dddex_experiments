{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d7bc07-6fe5-4427-9498-d9dd36cdcf9c",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3b6ee14-d0ab-47a7-ad24-ee4018560cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import pyreadr\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import os\n",
    "import copy\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# Import ML models\n",
    "import sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from dddex.levelSetKDEx_univariate import LevelSetKDEx, LevelSetKDEx_NN\n",
    "from dddex.wSAA import RandomForestWSAA, SampleAverageApproximation\n",
    "from dddex.crossValidation import QuantileCrossValidation, QuantileCrossValidationLSx, groupedTimeSeriesSplit\n",
    "from dddex.utils import generateFinalOutput\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Import Gurobi\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Optimization Module\n",
    "from DataDrivenPatientScheduling import PatientScheduling\n",
    "from DataDrivenPatientScheduling import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bbb55cd-5961-4ed1-9cad-9227a2b1136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the experiment\n",
    "experiment_setup = dict(\n",
    "\n",
    "    # Set paths\n",
    "    path_data = '/home/fesc/dddex/PatientScheduling/Data',\n",
    "    path_results = '/home/fesc/dddex/PatientScheduling/Data/Results',\n",
    "\n",
    "    # Set gurobi params\n",
    "    gurobi_params = {\n",
    "    \n",
    "        'LogToConsole': 0, \n",
    "        'Threads': 1\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Meta parameters\n",
    "    model_params = {\n",
    "        'random_state': 12345,\n",
    "        'n_jobs': 4,\n",
    "        'verbose': -1\n",
    "    },\n",
    "\n",
    "    # Hyper params search grid\n",
    "    hyper_params_grid = {\n",
    "        'num_leaves': [10, 20, 40, 80, 160, 320],\n",
    "        'max_depth': [-1, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'min_child_samples': [10, 20, 40, 80, 160, 320],\n",
    "        'learning_rate': [x/100 for x in range(5, 25, 5)],\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'subsample': [x/100 for x in range(5, 100, 5)],\n",
    "        'colsample_bytree': [x/100 for x in range(5, 100, 5)]\n",
    "    },    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Tuning params\n",
    "    tuning_params = {     \n",
    "        'n_iter': 100,\n",
    "        'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "        'return_train_score': True,\n",
    "        'refit': 'MSE',\n",
    "        'random_state': 12345,\n",
    "        'n_jobs': 8,\n",
    "        'verbose': 0\n",
    "    },    \n",
    "\n",
    "    # Bin size grid\n",
    "    binSizeGrid = {\n",
    "        'binSize': [x for x in range(100, 1000, 100)]\n",
    "    },\n",
    "    \n",
    "    # Tuning using random search\n",
    "    random_search = True,\n",
    "    \n",
    "    # Status printing\n",
    "    print_status = True,\n",
    "    \n",
    "    # Target quantiles\n",
    "    quantiles = np.array([0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.900, 0.975, 0.995])\n",
    "\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b364b3-8ec8-4a61-b538-0c52c6db182c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f86e0c5-0ed6-4e5c-81fe-fb6a5f9d9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Y_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/Y_data.csv')\n",
    "X_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/X_data.csv')\n",
    "ID_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/ID_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a32082-81de-4f17-b867-19d9b69659c3",
   "metadata": {},
   "source": [
    "# Tuning point estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a5e0870-504e-4a6c-8f0e-efdcfd4d0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "y_train = np.array(Y_data.loc[ID_data.train_test == 'train']).flatten()\n",
    "X_train = np.array(X_data.loc[ID_data.train_test == 'train'])\n",
    "ID_train = ID_data.loc[ID_data.train_test == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27280d11-90c7-4ab1-baa1-d241385f0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV folds\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "cv_folds = tscv.split(range(len(ID_train)))\n",
    "cv_folds = list(cv_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60a8e339-5a45-4c43-8331-d1fe66431b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor\n",
    "estimator = LGBMRegressor(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eadec27b-00db-4627-8a26-8b60c633d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... took 0:00:05\n"
     ]
    }
   ],
   "source": [
    "# Timer\n",
    "start_time = dt.datetime.now().replace(microsecond=0)\n",
    "st_exec = time.time()\n",
    "st_cpu = time.process_time() \n",
    "\n",
    "# Hyper params grid: ensure max features is not too large\n",
    "hyper_params_grid['colsample_bytree'] = [\n",
    "    colsample_bytree \n",
    "    for colsample_bytree \n",
    "    in hyper_params_grid.get('colsample_bytree', [0]) \n",
    "    if colsample_bytree <= X_train.shape[1]\n",
    "]\n",
    "\n",
    "# Base model\n",
    "estimator = LGBMRegressor(**model_params)  \n",
    "\n",
    "# Tuning approach\n",
    "if random_search:\n",
    "\n",
    "    # Random search CV\n",
    "    cv_search = RandomizedSearchCV(estimator=estimator,\n",
    "                                   cv=cv_folds,\n",
    "                                   param_distributions=hyper_params_grid,\n",
    "                                   **tuning_params)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Grid search SV\n",
    "    cv_search = GridSearchCV(estimator=estimator,\n",
    "                             cv=cv_folds,\n",
    "                             param_grid=hyper_params_grid,\n",
    "                             **tuning_params)\n",
    "\n",
    "# Fit the cv search\n",
    "cv_search.fit(X_train, y_train) \n",
    "\n",
    "# Status\n",
    "if print_status: \n",
    "    print('... took', dt.datetime.now().replace(microsecond=0) - start_time)      \n",
    "\n",
    "# Timer\n",
    "exec_time_sec = time.time()-st_exec\n",
    "cpu_time_sec = time.process_time()-st_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0116049e-9ed2-4efc-a680-151b5fa5faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV results\n",
    "best_hyper_params = cv_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52daf0-e6c0-4b3b-96db-5c1d71c01aab",
   "metadata": {},
   "source": [
    "# Tuning density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c734009-4bad-46d6-9b67-40ba9805a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSx model based on tuned point predictor\n",
    "LSKDEx = LevelSetKDEx(estimator = LGBMRegressor(**model_params, **best_hyper_params),\n",
    "                      weightsByDistance = False)\n",
    "\n",
    "# Set up bin size tuning\n",
    "LSxCV = QuantileCrossValidation(estimator = LSKDEx, \n",
    "                                cvFolds = cv_folds,\n",
    "                                parameterGrid = binSizeGrid,\n",
    "                                probs = quantiles,\n",
    "                                n_jobs = 8)\n",
    "\n",
    "# Tune bin-size\n",
    "LSxCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4699a8-aed0-4aa6-b9a2-8bd3769fe942",
   "metadata": {},
   "source": [
    "# Data-driven optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d75412-5d55-4f99-aa26-84bc74ec3d7b",
   "metadata": {},
   "source": [
    "For each patient $j=1,...,M$ for the current day $t$, we have $N_t$ historical samples. Given patient $j$ and the associated feature vector $\\boldsymbol{x}_{j,t}$, we have $N_t$ weights for these samples. With this, we have an approximation of the empirical probability distribution of surgery duration. We use the approximated distribution to draw $K$ scenarios for each patient $j=1,...,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "965925a6-b922-49a7-9663-cd8fb857773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'LSx_LGBM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7ea0e5c-1bc8-4c65-87ed-8bb623e57a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dca2d2c6-77ef-417a-ab6e-a586b53b53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Experiment\n",
    "exp = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "662db13c-3ff2-4952-84a5-7dbd533c05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_params = [\n",
    "    \n",
    "    {'CR': 0.10, 'c_waiting_time': 1, 'c_overtime': 9},\n",
    "    {'CR': 0.25, 'c_waiting_time': 1, 'c_overtime': 3},\n",
    "    {'CR': 0.50, 'c_waiting_time': 1, 'c_overtime': 1},\n",
    "    {'CR': 0.75, 'c_waiting_time': 3, 'c_overtime': 1},\n",
    "    {'CR': 0.90, 'c_waiting_time': 9, 'c_overtime': 1}\n",
    "\n",
    "]\n",
    "\n",
    "K = 10**4\n",
    "\n",
    "gurobi_params = {'LogToConsole': 0, 'Threads': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09793901-af6b-4865-ace3-df5887ed0fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100% (63/63 days) | Current time: 7:00:36\r"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# Timer\n",
    "start_time = dt.datetime.now().replace(microsecond=0)\n",
    "counter = 0\n",
    "\n",
    "# Progress\n",
    "print(\"Progress: \"+str(int(np.around(counter / len(dates), 2) * 100))+\"% (\"+str(counter)+\"/\"+str(len(dates))+\" days) |\",\n",
    "      \"Current time: \"+str(dt.datetime.now().replace(microsecond=0) - start_time), end='\\r')\n",
    "    \n",
    "# For each date in the test horizon\n",
    "for date in dates: \n",
    "        \n",
    "    # Progress\n",
    "    counter += 1\n",
    "                \n",
    "    # Train-test split\n",
    "    y_train, y_test = np.array(Y_data.loc[ID_data['date'] < date]).flatten(), np.array(Y_data.loc[ID_data['date'] == date]).flatten()\n",
    "    X_train, X_test = np.array(X_data.loc[ID_data['date'] < date]), np.array(X_data.loc[ID_data['date'] == date])\n",
    "    ID_train, ID_test = ID_data.loc[ID_data['date'] < date], ID_data.loc[ID_data['date'] == date]\n",
    "    \n",
    "    # Initialize weights model (best estimator from hyper param tuning)\n",
    "    LSKDEx = LSxCV.bestEstimator\n",
    "\n",
    "    # Fit weights model\n",
    "    LSKDEx.fit(X_train, y_train)\n",
    "\n",
    "    # Get time budget per area\n",
    "    historicalDurations = pd.DataFrame({\n",
    "        'duration': y_train, \n",
    "        'treatment': ID_train['treatment']\n",
    "    }).groupby('treatment').agg(\n",
    "        median_duration=('duration', np.median)\n",
    "    ).reset_index().rename(columns={'treatment': 'area'})\n",
    "\n",
    "    historicalDurations = dict(zip(historicalDurations.area, historicalDurations.median_duration))\n",
    "\n",
    "    # Data-driven optimization\n",
    "    result = exp.run_ddps(\n",
    "        weightsModel = LSKDEx, \n",
    "        X_test = X_test, \n",
    "        y_test = y_test, \n",
    "        date = date, \n",
    "        dates = ID_test['date'], \n",
    "        areas = ID_test['treatment'], \n",
    "        hist_durations = historicalDurations,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        K = K,\n",
    "        print_status = False\n",
    "    )\n",
    "    \n",
    "    # Add result\n",
    "    results = pd.concat([results, result])\n",
    "    \n",
    "    # Progress\n",
    "    print(\"Progress: \"+str(int(np.around(counter / len(dates), 2) * 100))+\"% (\"+str(counter)+\"/\"+str(len(dates))+\" days) |\",\n",
    "          \"Current time: \"+str(dt.datetime.now().replace(microsecond=0) - start_time), end='\\r')\n",
    "    \n",
    "# Finalize\n",
    "results = results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28a39b6b-63ab-44e9-bb65-5eb282143075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save results\n",
    "results.to_csv(path_results+\"/\"+model_name+\"_K\"+str(K)+\".csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d7d8a-be36-4452-8174-cb15323e11d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a8c5c-2312-452c-8ac4-6020a63bad03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f4690-97ac-4ee4-8c9b-a38d4f56ea1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28102a9-51db-4a6f-884d-b17ef5e5fb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616766cb-84ab-474d-9b35-e5af7c04aa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c980e93-d5f1-4f1a-8e15-ee0ba7b9098d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
