{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d7bc07-6fe5-4427-9498-d9dd36cdcf9c",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b6ee14-d0ab-47a7-ad24-ee4018560cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import pyreadr\n",
    "import pickle\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "import copy\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Import ML models\n",
    "import sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from dddex.levelSetKDEx_univariate import LevelSetKDEx, LevelSetKDEx_NN\n",
    "from dddex.wSAA import RandomForestWSAA, SampleAverageApproximation\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Import Gurobi\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Data-Driven Patient Scheduling modules\n",
    "from DataDrivenPatientScheduling.WeightsModel import WeightsModel\n",
    "from DataDrivenPatientScheduling.Experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a439c0ba-610c-48fd-86c5-01f8ee51483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories and model names\n",
    "directories_setup = dict(\n",
    "\n",
    "    # Paths\n",
    "    path_data = '/home/fesc/dddex/PatientScheduling/Data',\n",
    "    path_models = '/home/fesc/dddex/PatientScheduling/Data/Models',\n",
    "    path_results = '/home/fesc/dddex/PatientScheduling/Data/Results',\n",
    "    \n",
    "    # Models\n",
    "    LSx_LGBM = 'LSx_LGBM',\n",
    "    LSx_NN_LGBM = 'LSx_NN_LGBM',\n",
    "    wSAA_RF = 'wSAA_RF',\n",
    "    SAA_by_area = 'SAA_by_area',\n",
    "    SAA = 'SAA'\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(directories_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3be33-7978-45d7-98e9-d7bfde0bc1d0",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97298486-a92d-4b67-87f5-0e00ddb8e8d1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75fec1-c62a-4d64-a178-3dbbf1b0468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "y_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/y_data.csv')\n",
    "X_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/X_data.csv')\n",
    "ID_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/ID_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef3b52-1077-492b-93f3-22aba3428b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (for initial model training)\n",
    "y_train = np.array(y_data.loc[ID_data.train_test == 'train']).flatten()\n",
    "X_train = np.array(X_data.loc[ID_data.train_test == 'train'])\n",
    "ID_train = ID_data.loc[ID_data.train_test == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f9c4c-0a51-485b-847f-ea8146c317a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV folds\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "cv_folds = tscv.split(range(len(ID_train)))\n",
    "cv_folds = list(cv_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762abee3-ac31-4912-a8ff-b2f9b6c035d1",
   "metadata": {},
   "source": [
    "## (a) LSx - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb55cd-5961-4ed1-9cad-9227a2b1136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "model_setup = dict(\n",
    "\n",
    "    ## Point estimator\n",
    "    point_estimator_params = dict(\n",
    "\n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 4,\n",
    "            'verbose': -1\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 1000,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "       # Hyper params search grid\n",
    "       hyper_params_grid = {\n",
    "            'num_leaves': [x for x in range(5, 500, 5)],\n",
    "            'max_depth': [-1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_child_samples': [x for x in range(5, 500, 5)],\n",
    "            'learning_rate': [x/100 for x in range(1, 20+1, 1)],\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'subsample': [x/100 for x in range(5, 100+1, 5)],\n",
    "            'colsample_bytree': [x/100 for x in range(5, 100, 5)]\n",
    "        },  \n",
    "    ),\n",
    "          \n",
    " \n",
    "    ## Density estimator\n",
    "    density_estimator_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'weightsByDistance': False\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'probs': [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "            'n_jobs': 8,\n",
    "        },    \n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'binSize': [x for x in range(10, 500, 10)]\n",
    "        },\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all model variables visible locally\n",
    "locals().update(model_setup)\n",
    "\n",
    "# Initialize modules\n",
    "weightsmodel = WeightsModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a32082-81de-4f17-b867-19d9b69659c3",
   "metadata": {},
   "source": [
    "### Tune point estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9829d7c-9b90-4247-b5ef-db48f5722c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(point_estimator_params)\n",
    "\n",
    "# Point estimator\n",
    "point_estimator = LGBMRegressor(**model_params)\n",
    "\n",
    "# Tune point estimator\n",
    "point_estimator = weightsmodel.tune_point_estimator(X_train, y_train, point_estimator, cv_folds, \n",
    "                                                    hyper_params_grid, tuning_params, random_search=True, \n",
    "                                                    print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52daf0-e6c0-4b3b-96db-5c1d71c01aab",
   "metadata": {},
   "source": [
    "### Tune density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64313726-84f9-4279-9a0c-9bd79fe81a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(density_estimator_params)\n",
    "\n",
    "# Density estimator\n",
    "density_estimator = LevelSetKDEx(estimator = point_estimator, **model_params)\n",
    "\n",
    "# Tune density estimator\n",
    "density_estimator = weightsmodel.tune_density_estimator(X_train, y_train, density_estimator, cv_folds, \n",
    "                                                        hyper_params_grid, tuning_params, random_search=False, \n",
    "                                                        print_time=True)\n",
    "\n",
    "# Save\n",
    "save = dump(density_estimator, path_models+'/density_estimator_'+LSx_LGBM+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91554ca1-64e8-46d3-acb4-c9f8ad32b576",
   "metadata": {},
   "source": [
    "## (b) LSx NN - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15eccde-17b7-4727-8084-c3e47d9acb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "model_setup = dict(\n",
    "        \n",
    "    ## Point estimator\n",
    "    point_estimator_params = dict(\n",
    "\n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 4,\n",
    "            'verbose': -1\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 1000,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "       # Hyper params search grid\n",
    "       hyper_params_grid = {\n",
    "            'num_leaves': [x for x in range(5, 500, 5)],\n",
    "            'max_depth': [-1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_child_samples': [x for x in range(5, 500, 5)],\n",
    "            'learning_rate': [x/100 for x in range(1, 20+1, 1)],\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'subsample': [x/100 for x in range(5, 100+1, 5)],\n",
    "            'colsample_bytree': [x/100 for x in range(5, 100, 5)]\n",
    "        },  \n",
    "    ),\n",
    "          \n",
    " \n",
    "    ## Density estimator\n",
    "    density_estimator_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'weightsByDistance': True\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'probs': [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "            'n_jobs': 8,\n",
    "        },    \n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'binSize': [x for x in range(10, 500, 10)]\n",
    "        },\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all model variables visible locally\n",
    "locals().update(model_setup)\n",
    "\n",
    "# Initialize modules\n",
    "weightsmodel = WeightsModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66e371-ed1b-4d55-a7db-7ae11977d8b5",
   "metadata": {},
   "source": [
    "### Tune point estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48f056-f186-45b5-aeca-9c992fb48b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(point_estimator_params)\n",
    "\n",
    "# Point estimator\n",
    "point_estimator = LGBMRegressor(**model_params)\n",
    "\n",
    "# Tune point estimator\n",
    "point_estimator = weightsmodel.tune_point_estimator(X_train, y_train, point_estimator, cv_folds, hyper_params_grid, \n",
    "                                                    tuning_params, random_search=True, print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19e9a3-d7b4-4a20-b5ef-a175510e41b4",
   "metadata": {},
   "source": [
    "### Tune density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d8d36-0bdc-4c51-8cd6-b95aa79302e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(density_estimator_params)\n",
    "\n",
    "# Density estimator\n",
    "density_estimator = LevelSetKDEx_NN(estimator = point_estimator, **model_params)\n",
    "\n",
    "# Tune density estimator\n",
    "density_estimator = weightsmodel.tune_density_estimator(X_train, y_train, density_estimator, cv_folds, hyper_params_grid, \n",
    "                                                        tuning_params, random_search=False, print_time=True)\n",
    "\n",
    "# Save\n",
    "save = dump(density_estimator, path_models+'/density_estimator_'+LSx_NN_LGBM+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0994-c480-4d57-965b-53225fb15729",
   "metadata": {},
   "source": [
    "## (c) wSAA - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db864d2-8645-477b-a4fe-35424187f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "model_setup = dict(\n",
    "        \n",
    "    ## Density estimator\n",
    "    density_estimator_params = dict(\n",
    "\n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'probs': [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "            'nIter': 1000,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8\n",
    "        },    \n",
    "\n",
    "       # Hyper params search grid\n",
    "       hyper_params_grid = {\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_samples_split': [x for x in range(5, 500, 5)],\n",
    "            'min_samples_leaf': [x for x in range(5, 500, 5)],\n",
    "            'max_features': [x for x in range(5, 100, 5)],\n",
    "            'max_leaf_nodes': [x for x in range(5, 500, 5)],\n",
    "            'min_impurity_decrease': [x/100 for x in range(1, 20+1, 1)],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [x/100 for x in range(5, 100+1, 5)]           \n",
    "        },          \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all model variables visible locally\n",
    "locals().update(model_setup)\n",
    "\n",
    "# Initialize modules\n",
    "weightsmodel = WeightsModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061af593-d134-4d3e-9bfa-de2e9978c427",
   "metadata": {},
   "source": [
    "### Tune density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ea520-3dc8-4f61-83e9-2775ef28d363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(density_estimator_params)\n",
    "\n",
    "# Density estimator\n",
    "density_estimator = RandomForestWSAA(**model_params)\n",
    "\n",
    "# Tune density estimator\n",
    "density_estimator = weightsmodel.tune_density_estimator(X_train, y_train, density_estimator, cv_folds, hyper_params_grid, \n",
    "                                                        tuning_params, random_search=True, print_time=True)\n",
    "\n",
    "# Save\n",
    "save = dump(density_estimator, path_models+'/density_estimator_'+wSAA_RF+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeace3a-6f38-4f6a-8688-b7b776ac6f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc5761-d639-419b-849d-f4b76c3b18b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca37d7e-d4fd-4cad-8a53-10b55c634dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b878cf-7ca6-41ee-8e1e-c352b98c0837",
   "metadata": {},
   "source": [
    "# Data-driven optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477db6f1-26e5-4db1-9989-3c98ff9d74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the experiment\n",
    "optimization_params = dict(\n",
    "\n",
    "    # Gurobi params\n",
    "    gurobi_params = {\n",
    "\n",
    "        'LogToConsole': 0, \n",
    "        'Threads': 2\n",
    "\n",
    "    },\n",
    "\n",
    "    # Cost params\n",
    "    cost_params = [\n",
    "\n",
    "        {'c_waiting_time': 1, 'c_overtime': 1},\n",
    "        {'c_waiting_time': 1, 'c_overtime': 2},\n",
    "        {'c_waiting_time': 1, 'c_overtime': 3}\n",
    "        \n",
    "    ],\n",
    "\n",
    "    # Number of scenarios\n",
    "    K = [10**2, 10**3, 10**4],\n",
    "\n",
    "    # Time budget multiplier\n",
    "    rho = [0.85, 1.00, 1.15],\n",
    "\n",
    "    # n parallel jobs\n",
    "    n_jobs = 32\n",
    "\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(optimization_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2447cd97-01fe-475b-92aa-f7d55a25a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of run\n",
    "#run_suffix = 'xArea'\n",
    "#run_suffix = 'xCapacity'\n",
    "run_suffix = 'xCapacity_xArea'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89425ac7-64a3-4c1f-82a1-1f4d5489d862",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7bff1e0-75ad-488c-86aa-3d7eccda5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "y_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/y_data.csv')\n",
    "X_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/X_data.csv')\n",
    "ID_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/ID_data.csv')\n",
    "\n",
    "room_assignments = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/room_assignments_'+run_suffix+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c57af5d-56c9-4ea0-97f3-78caabde988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure room assignments exactly match date, area, patient_id (i.e., each case)\n",
    "rooms = []\n",
    "for date, area, patient_id in zip(ID_data.date, ID_data.area, ID_data.patient_id):\n",
    "    \n",
    "    sel = (room_assignments.date == date) & (room_assignments.area == area) & (room_assignments.patient_id == patient_id)\n",
    "    \n",
    "    rooms += [room_assignments.loc[sel].room.item()]\n",
    "    \n",
    "ID_data['room'] = copy.deepcopy(rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4699a8-aed0-4aa6-b9a2-8bd3769fe942",
   "metadata": {},
   "source": [
    "## (a) LSx - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183de39f-21c6-49a3-8696-c90aff592dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 64/64 [1:34:15<00:00, 88.36s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1:34:15\n",
      ">> Execution time: 5655.0 seconds\n",
      ">> CPU time: 4.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Load density estimator\n",
    "density_estimator = load(path_models+'/density_estimator_'+LSx_LGBM+'.joblib')\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        rooms = ID_data['room'],\n",
    "        weightsModel = density_estimator,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+LSx_LGBM+'_'+run_suffix+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd948c-353b-4d7b-8a74-b1d1df043f80",
   "metadata": {},
   "source": [
    "## (b) LSx NN - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1bd7bf-86ec-4d8e-8ce0-d38de014018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 64/64 [1:45:05<00:00, 98.52s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1:45:06\n",
      ">> Execution time: 6306.0 seconds\n",
      ">> CPU time: 8.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Load density estimator\n",
    "density_estimator = load(path_models+'/density_estimator_'+LSx_NN_LGBM+'.joblib')\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        rooms = ID_data['room'],\n",
    "        weightsModel = density_estimator,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+LSx_NN_LGBM+'_'+run_suffix+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f069dd0-ab16-48a7-af14-93b73b8aed8a",
   "metadata": {},
   "source": [
    "## (c) wSAA RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb95e309-5609-4647-9b5a-772dcf8a2d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 64/64 [1:44:03<00:00, 97.55s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1:44:03\n",
      ">> Execution time: 6243.0 seconds\n",
      ">> CPU time: 7.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Load density estimator\n",
    "density_estimator = load(path_models+'/density_estimator_'+wSAA_RF+'.joblib')\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        rooms = ID_data['room'],\n",
    "        weightsModel = density_estimator,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+wSAA_RF+'_'+run_suffix+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f10f7-c167-4ba1-a520-de3d6bfe342e",
   "metadata": {},
   "source": [
    "## (d) SAA by treatment area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c08fa8-f93a-478a-bc68-15325646ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 64/64 [1:43:32<00:00, 97.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1:43:33\n",
      ">> Execution time: 6213.0 seconds\n",
      ">> CPU time: 3.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        rooms = ID_data['room'],\n",
    "        weightsModel = SampleAverageApproximation(),\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+SAA_by_area+'_'+run_suffix+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f4b8a-f956-41b1-97a3-36e7d143a19f",
   "metadata": {},
   "source": [
    "## (e) SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea61bcc-5968-44b6-87c3-a74da5e53b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 64/64 [1:17:46<00:00, 72.91s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1:17:47\n",
      ">> Execution time: 4667.0 seconds\n",
      ">> CPU time: 2.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        rooms = ID_data['room'],\n",
    "        weightsModel = None,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+SAA+'_'+run_suffix+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642daf90-cedf-4806-9330-293456ca1c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f5851-13a7-48f0-b4dd-96c77890b0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec52c0-e185-434f-95cf-aa890c918662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d911ac4-7211-4ff8-ae8b-84e51f2b1e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4082a1-b483-4cec-8090-4da822db82fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
