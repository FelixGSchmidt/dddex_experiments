{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d7bc07-6fe5-4427-9498-d9dd36cdcf9c",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6ee14-d0ab-47a7-ad24-ee4018560cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import pyreadr\n",
    "import pickle\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "import copy\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Import ML models\n",
    "import sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from dddex.levelSetKDEx_univariate import LevelSetKDEx, LevelSetKDEx_NN\n",
    "from dddex.wSAA import RandomForestWSAA, SampleAverageApproximation\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Import Gurobi\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Optimization Module\n",
    "from DataDrivenPatientScheduling import WeightsModel\n",
    "from DataDrivenPatientScheduling import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439c0ba-610c-48fd-86c5-01f8ee51483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories and model names\n",
    "directories_setup = dict(\n",
    "\n",
    "    # Paths\n",
    "    path_data = '/home/fesc/dddex/PatientScheduling/Data',\n",
    "    path_models = '/home/fesc/dddex/PatientScheduling/Data/Models',\n",
    "    path_results = '/home/fesc/dddex/PatientScheduling/Data/Results',\n",
    "    \n",
    "    # Models\n",
    "    LSx_LGBM = 'LSx_LGBM',\n",
    "    LSx_NN_LGBM = 'LSx_NN_LGBM',\n",
    "    wSAA_RF = 'wSAA_RF',\n",
    "    SAA = 'SAA'\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(directories_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97298486-a92d-4b67-87f5-0e00ddb8e8d1",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75fec1-c62a-4d64-a178-3dbbf1b0468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Y_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/Y_data.csv')\n",
    "X_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/X_data.csv')\n",
    "ID_data = pd.read_csv('/home/fesc/dddex/PatientScheduling/Data/ID_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f9c4c-0a51-485b-847f-ea8146c317a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (for initial model training)\n",
    "y_train = np.array(Y_data.loc[ID_data.train_test == 'train']).flatten()\n",
    "X_train = np.array(X_data.loc[ID_data.train_test == 'train'])\n",
    "ID_train = ID_data.loc[ID_data.train_test == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7892d-0c70-4899-89a4-618889f56931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV folds\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "cv_folds = tscv.split(range(len(ID_train)))\n",
    "cv_folds = list(cv_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3be33-7978-45d7-98e9-d7bfde0bc1d0",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762abee3-ac31-4912-a8ff-b2f9b6c035d1",
   "metadata": {},
   "source": [
    "## (a) LSx - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb55cd-5961-4ed1-9cad-9227a2b1136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "model_setup = dict(\n",
    "\n",
    "    ## Point estimator\n",
    "    point_estimator_params = dict(\n",
    "\n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 4,\n",
    "            'verbose': -1\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 200,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "       # Hyper params search grid\n",
    "       hyper_params_grid = {\n",
    "            'num_leaves': [x for x in range(5, 500, 5)],\n",
    "            'max_depth': [-1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_child_samples': [x for x in range(5, 500, 5)],\n",
    "            'learning_rate': [x/100 for x in range(1, 20+1, 1)],\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'subsample': [x/100 for x in range(5, 100+1, 5)],\n",
    "            'colsample_bytree': [x/100 for x in range(5, 100, 5)]\n",
    "        },  \n",
    "    ),\n",
    "          \n",
    " \n",
    "    ## Density estimator\n",
    "    density_estimator_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'weightsByDistance': False\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'probs': [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "            # 'probs': sorted(list(set(np.concatenate([np.array([0.005, 0.025, 0.165, 0.25, 0.50, 0.835, 0.75, 0.975, 0.995]), \n",
    "            #                                          np.arange(1, 100, 1) / 100])))),\n",
    "            'n_jobs': 8,\n",
    "        },    \n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'binSize': [x for x in range(10, 500, 10)]\n",
    "        },\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all model variables visible locally\n",
    "locals().update(model_setup)\n",
    "\n",
    "# Initialize modules\n",
    "weightsmodel = WeightsModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a32082-81de-4f17-b867-19d9b69659c3",
   "metadata": {},
   "source": [
    "### Tune point estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9829d7c-9b90-4247-b5ef-db48f5722c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(point_estimator_params)\n",
    "\n",
    "# Point estimator\n",
    "point_estimator = LGBMRegressor(**model_params)\n",
    "\n",
    "# Tune point estimator\n",
    "point_estimator = weightsmodel.tune_point_estimator(X_train, y_train, point_estimator, cv_folds, hyper_params_grid, \n",
    "                                                    tuning_params, random_search=True, print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52daf0-e6c0-4b3b-96db-5c1d71c01aab",
   "metadata": {},
   "source": [
    "### Tune density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64313726-84f9-4279-9a0c-9bd79fe81a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(density_estimator_params)\n",
    "\n",
    "# Density estimator\n",
    "density_estimator = LevelSetKDEx(estimator = point_estimator, **model_params)\n",
    "\n",
    "# Tune density estimator\n",
    "density_estimator = weightsmodel.tune_density_estimator(X_train, y_train, density_estimator, cv_folds, hyper_params_grid, \n",
    "                                                        tuning_params, random_search=False, print_time=True)\n",
    "\n",
    "# Save\n",
    "save = dump(density_estimator, path_models+'/density_estimator_'+LSx_LGBM+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91554ca1-64e8-46d3-acb4-c9f8ad32b576",
   "metadata": {},
   "source": [
    "## (b) LSx NN - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15eccde-17b7-4727-8084-c3e47d9acb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "model_setup = dict(\n",
    "        \n",
    "    ## Point estimator\n",
    "    point_estimator_params = dict(\n",
    "\n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 4,\n",
    "            'verbose': -1\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 200,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "       # Hyper params search grid\n",
    "       hyper_params_grid = {\n",
    "            'num_leaves': [x for x in range(5, 500, 5)],\n",
    "            'max_depth': [-1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_child_samples': [x for x in range(5, 500, 5)],\n",
    "            'learning_rate': [x/100 for x in range(1, 20+1, 1)],\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'subsample': [x/100 for x in range(5, 100+1, 5)],\n",
    "            'colsample_bytree': [x/100 for x in range(5, 100, 5)]\n",
    "        },  \n",
    "    ),\n",
    "          \n",
    " \n",
    "    ## Density estimator\n",
    "    density_estimator_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'weightsByDistance': True\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'probs': [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "            # 'probs': sorted(list(set(np.concatenate([np.array([0.005, 0.025, 0.165, 0.25, 0.50, 0.835, 0.75, 0.975, 0.995]), \n",
    "            #                                          np.arange(1, 100, 1) / 100])))),\n",
    "            'n_jobs': 8,\n",
    "        },    \n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'binSize': [x for x in range(10, 500, 10)]\n",
    "        },\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all model variables visible locally\n",
    "locals().update(model_setup)\n",
    "\n",
    "# Initialize modules\n",
    "weightsmodel = WeightsModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66e371-ed1b-4d55-a7db-7ae11977d8b5",
   "metadata": {},
   "source": [
    "### Tune point estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48f056-f186-45b5-aeca-9c992fb48b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(point_estimator_params)\n",
    "\n",
    "# Point estimator\n",
    "point_estimator = LGBMRegressor(**model_params)\n",
    "\n",
    "# Tune point estimator\n",
    "point_estimator = weightsmodel.tune_point_estimator(X_train, y_train, point_estimator, cv_folds, hyper_params_grid, \n",
    "                                                    tuning_params, random_search=True, print_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19e9a3-d7b4-4a20-b5ef-a175510e41b4",
   "metadata": {},
   "source": [
    "### Tune density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d8d36-0bdc-4c51-8cd6-b95aa79302e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(density_estimator_params)\n",
    "\n",
    "# Density estimator\n",
    "density_estimator = LevelSetKDEx_NN(estimator = point_estimator, **model_params)\n",
    "\n",
    "# Tune density estimator\n",
    "density_estimator = weightsmodel.tune_density_estimator(X_train, y_train, density_estimator, cv_folds, hyper_params_grid, \n",
    "                                                        tuning_params, random_search=False, print_time=True)\n",
    "\n",
    "# Save\n",
    "save = dump(density_estimator, path_models+'/density_estimator_'+LSx_NN_LGBM+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0994-c480-4d57-965b-53225fb15729",
   "metadata": {},
   "source": [
    "## (c) wSAA - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db864d2-8645-477b-a4fe-35424187f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "model_setup = dict(\n",
    "        \n",
    "    ## Density estimator\n",
    "    density_estimator_params = dict(\n",
    "\n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Tuning meta params\n",
    "        tuning_params = {     \n",
    "            'probs': [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "            # 'probs': sorted(list(set(np.concatenate([np.array([0.005, 0.025, 0.165, 0.25, 0.50, 0.835, 0.75, 0.975, 0.995]), \n",
    "            #                                          np.arange(1, 100, 1) / 100])))),\n",
    "            'nIter': 200,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8\n",
    "        },    \n",
    "\n",
    "       # Hyper params search grid\n",
    "       hyper_params_grid = {\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_samples_split': [x for x in range(5, 500, 5)],\n",
    "            'min_samples_leaf': [x for x in range(5, 500, 5)],\n",
    "            'max_features': [x for x in range(5, 100, 5)],\n",
    "            'max_leaf_nodes': [x for x in range(5, 500, 5)],\n",
    "            'min_impurity_decrease': [x/100 for x in range(1, 20+1, 1)],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [x/100 for x in range(5, 100+1, 5)]           \n",
    "        },          \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all model variables visible locally\n",
    "locals().update(model_setup)\n",
    "\n",
    "# Initialize modules\n",
    "weightsmodel = WeightsModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061af593-d134-4d3e-9bfa-de2e9978c427",
   "metadata": {},
   "source": [
    "### Tune density estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ea520-3dc8-4f61-83e9-2775ef28d363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(density_estimator_params)\n",
    "\n",
    "# Density estimator\n",
    "density_estimator = RandomForestWSAA(**model_params)\n",
    "\n",
    "# Tune density estimator\n",
    "density_estimator = weightsmodel.tune_density_estimator(X_train, y_train, density_estimator, cv_folds, hyper_params_grid, \n",
    "                                                        tuning_params, random_search=True, print_time=True)\n",
    "\n",
    "# Save\n",
    "save = dump(density_estimator, path_models+'/density_estimator_'+wSAA_RF+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeace3a-6f38-4f6a-8688-b7b776ac6f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc5761-d639-419b-849d-f4b76c3b18b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca37d7e-d4fd-4cad-8a53-10b55c634dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b878cf-7ca6-41ee-8e1e-c352b98c0837",
   "metadata": {},
   "source": [
    "# Data-driven optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b1b6f-7a96-4b1b-8b50-39f3579e9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the experiment\n",
    "optimization_params = dict(\n",
    "\n",
    "    # Gurobi params\n",
    "    gurobi_params = {\n",
    "\n",
    "        'LogToConsole': 0, \n",
    "        'Threads': 2\n",
    "\n",
    "    },\n",
    "\n",
    "    # Cost params\n",
    "    cost_params = [\n",
    "\n",
    "        {'CR': 0.10, 'c_waiting_time': 1, 'c_overtime': 9},\n",
    "        {'CR': 0.25, 'c_waiting_time': 1, 'c_overtime': 3},\n",
    "        {'CR': 0.50, 'c_waiting_time': 1, 'c_overtime': 1},\n",
    "        {'CR': 0.75, 'c_waiting_time': 3, 'c_overtime': 1},\n",
    "        {'CR': 0.90, 'c_waiting_time': 9, 'c_overtime': 1}\n",
    "\n",
    "    ],\n",
    "\n",
    "    # Number of scenarios\n",
    "    K = 10**1,\n",
    "\n",
    "    # Time budget multiplier\n",
    "    rho = 1.15,\n",
    "\n",
    "    # n parallel jobs\n",
    "    n_jobs = 32\n",
    "\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(optimization_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4699a8-aed0-4aa6-b9a2-8bd3769fe942",
   "metadata": {},
   "source": [
    "## (a) LSx - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183de39f-21c6-49a3-8696-c90aff592dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Load density estimator\n",
    "density_estimator = load(path_models+'/density_estimator_'+LSx_LGBM+'.joblib')\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(Y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        weightsModel = density_estimator,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+LSx_LGBM+'_K'+str(K)+'_rho'+str(rho)+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd948c-353b-4d7b-8a74-b1d1df043f80",
   "metadata": {},
   "source": [
    "## (b) LSx NN - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bd7bf-86ec-4d8e-8ce0-d38de014018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Load density estimator\n",
    "density_estimator = load(path_models+'/density_estimator_'+LSx_NN_LGBM+'.joblib')\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(Y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        weightsModel = density_estimator,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+LSx_NN_LGBM+'_K'+str(K)+'_rho'+str(rho)+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f069dd0-ab16-48a7-af14-93b73b8aed8a",
   "metadata": {},
   "source": [
    "## (c) wSAA RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95e309-5609-4647-9b5a-772dcf8a2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Load density estimator\n",
    "density_estimator = load(path_models+'/density_estimator_'+wSAA_RF+'.joblib')\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(Y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        weightsModel = density_estimator,\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+wSAA_RF+'_K'+str(K)+'_rho'+str(rho)+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f10f7-c167-4ba1-a520-de3d6bfe342e",
   "metadata": {},
   "source": [
    "## (d) SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c08fa8-f93a-478a-bc68-15325646ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize modules\n",
    "experiment = Experiment()\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(Y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        weightsModel = SampleAverageApproximation(),\n",
    "        cost_params = cost_params,\n",
    "        gurobi_params = gurobi_params,\n",
    "        K = K,\n",
    "        rho = rho,\n",
    "        print_status = False) for date in dates)\n",
    "    \n",
    "# Finalize\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "# Save results\n",
    "results.to_csv(path_results+'/'+SAA+'_K'+str(K)+'_rho'+str(rho)+'.csv', sep=',', index=False)\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923be30-93c6-4893-85c1-6d5db27e480e",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d179b-0e73-4da0-ae1b-6438a3b668b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_LSx_LGBM = pd.read_csv(path_results+'/'+LSx_LGBM+'_K'+str(K)+'_rho'+str(rho)+'.csv')\n",
    "results_LSx_LGBM['model'] = 'LSx_LGBM'\n",
    "\n",
    "results_LSx_NN_LGBM = pd.read_csv(path_results+'/'+LSx_NN_LGBM+'_K'+str(K)+'_rho'+str(rho)+'.csv')\n",
    "results_LSx_NN_LGBM['model'] = 'LSx_LGBM_NN'\n",
    "\n",
    "results_wSAA_RF = pd.read_csv(path_results+'/'+wSAA_RF+'_K'+str(K)+'_rho'+str(rho)+'.csv')\n",
    "results_wSAA_RF['model'] = 'wSAA_RF'\n",
    "\n",
    "results_SAA = pd.read_csv(path_results+'/'+SAA+'_K'+str(K)+'_rho'+str(rho)+'.csv')\n",
    "results_SAA['model'] = 'SAA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892bbe6-3274-407d-99d9-12fa8ba2c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "results = pd.concat([\n",
    "    results_LSx_LGBM[['model', 'date', 'area', 'CR', 'cost', 'overtime', 'waiting_time']],\n",
    "    results_LSx_NN_LGBM[['model', 'date', 'area', 'CR', 'cost', 'overtime', 'waiting_time']],\n",
    "    results_wSAA_RF[['model', 'date', 'area', 'CR', 'cost', 'overtime', 'waiting_time']]\n",
    "])\n",
    "\n",
    "results = pd.merge(\n",
    "    results, \n",
    "    results_SAA[['date', 'area', 'CR', 'cost', 'overtime', 'waiting_time']],\n",
    "    on=['date', 'area', 'CR'],\n",
    "    suffixes=('', '_SAA')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9106a0e-688e-4dc6-8f6c-4659ea8962d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of prescriptiveness\n",
    "results['pq'] = results.cost / results.cost_SAA\n",
    "results.loc[results.cost == results.cost_SAA, 'pq'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ef7ce-d919-4ba5-9aeb-57a3bec4c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results.to_csv(path_results+'/results_K'+str(K)+'_rho'+str(rho)+'.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec52c0-e185-434f-95cf-aa890c918662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ec0ff-8c80-4a9a-b1de-f93d728d5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d911ac4-7211-4ff8-ae8b-84e51f2b1e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4082a1-b483-4cec-8090-4da822db82fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231c4f9-0f63-4384-abfd-0b88259b5267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d8fd27-f90d-4d52-95c7-a5503f8588bf",
   "metadata": {},
   "source": [
    "# <<<<<<<<<< ARCHIVE >>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199ffcf-c7a6-4503-81be-c2e1e2e13d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884a836-88db-48ea-bffa-593b7ecb0150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a8410-1e9d-492c-a5d6-3f4696acfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysze (median)\n",
    "results.groupby(['CR', 'area', 'model']).agg(pq=('pq', np.median)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7f4f5-1e9d-4619-bb53-ed508d790374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... for selected CR\n",
    "results.loc[results.CR.isin([0.10, 0.25, 0.50])].groupby(['CR', 'area', 'model']).agg(pq=('pq', np.median)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd58b13-9533-4624-b4df-482decfc168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... for selected CR and area\n",
    "results.loc[\n",
    "    results.CR.isin([0.10, 0.25, 0.50]) & \n",
    "    total.area.isin(['Bereich_Gastroskopie', 'Bereich_Koloskopie'])].groupby(['CR', 'area', 'model']).agg(\n",
    "    pq=('pq', np.median)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aaeae7-bbcc-427c-85da-699a403592eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysze (total) --- NEW\n",
    "total = results.groupby(['CR', 'area', 'model']).agg(total_cost=('cost', sum), total_cost_SAA=('cost_SAA', sum)).reset_index()\n",
    "total['pq'] = total.total_cost / total.total_cost_SAA\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0459b17-6068-47e0-903d-d99ce3ea3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysze (total)\n",
    "total = results.groupby(['CR', 'area', 'model']).agg(total_cost=('cost', sum), total_cost_SAA=('cost_SAA', sum)).reset_index()\n",
    "total['pq'] = total.total_cost / total.total_cost_SAA\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18fb10-869f-4c2f-94cd-57d94d54b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... for selected CR\n",
    "total.loc[total.CR.isin([0.10, 0.25, 0.50])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48b88e-5f2c-428d-a9ff-eb3c864bd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... for selected CR and area\n",
    "total.loc[total.CR.isin([0.10, 0.25, 0.50]) & total.area.isin(['Bereich_Gastroskopie', 'Bereich_Koloskopie'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025b33d-3509-4af3-a438-09bff2380cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... for selected CR and area --- NEW\n",
    "total.loc[total.CR.isin([0.10, 0.25, 0.50]) & total.area.isin(['Bereich_Gastroskopie', 'Bereich_Koloskopie'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6831d6d6-d4f1-40c9-b736-def280662fe5",
   "metadata": {},
   "source": [
    "# Scaled pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacb234-6d94-4998-9ff5-19385a1c0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ... \n",
    "def predict_quantiles(X, y, date, dates, areas, weightsModel = None, \n",
    "             quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "             print_status = False):\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        X: ...\n",
    "        y: ...\n",
    "        date: ...\n",
    "        dates: ...\n",
    "        areas: ...\n",
    "        weightsModel = None: ...\n",
    "        quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.900, 0.975, 0.995]: ...\n",
    "        print_status = True: ...\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        results(pd.DataFrame): ...\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Train-test split\n",
    "    y_train, y_test = y[dates < date].flatten(), y[dates == date].flatten()\n",
    "    X_train, X_test = X[dates < date], X[dates == date]\n",
    "    dates_train, dates_test = dates[dates < date], dates[dates == date]\n",
    "    areas_train, areas_test = areas[dates < date], areas[dates == date]\n",
    "\n",
    "    # Fit weights model\n",
    "    weightsModel.fit(X_train, y_train)\n",
    "\n",
    "    # Initialize\n",
    "    results = {}\n",
    "\n",
    "    # For each area \n",
    "    for area in list(set(areas_test)):\n",
    "\n",
    "        # Select test data for current area\n",
    "        y_test_ = y_test[area == areas_test]\n",
    "        X_test_ = X_test[area == areas_test]\n",
    "\n",
    "        # Fit SAA on data of current area\n",
    "        SAA = SampleAverageApproximation()\n",
    "        SAA.fit(y_train[area == areas_train])\n",
    "\n",
    "        # Predict quantiles for each patient\n",
    "        q_hat = weightsModel.predict(X_test_, probs=quantiles, outputAsDf=False)\n",
    "        q_hat_saa = SAA.predict(X_test_, probs=quantiles, outputAsDf=False)\n",
    "\n",
    "        # Add to results\n",
    "        results[area] = pd.merge(\n",
    "            left = pd.concat(pd.DataFrame({'prob': p, 'q_hat': q}) for p, q in q_hat.items()).reset_index(names='j'),\n",
    "            right = pd.concat(pd.DataFrame({'prob': p, 'q_hat_saa': q, 'y': y_test_}) for p, q in q_hat_saa.items()).reset_index(names='j'),\n",
    "            on = ['prob', 'j']\n",
    "        )\n",
    "        \n",
    "    # Finalize\n",
    "    results = pd.concat(results).reset_index(names=['area', '']).drop(columns='')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900744a-3ff4-421c-8a24-92d73473b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params\n",
    "locals().update(optimization_params)\n",
    "\n",
    "# Test dates\n",
    "dates = pd.Series(list(set(ID_data.loc[ID_data.train_test == 'test', 'date']))).sort_values()\n",
    "\n",
    "# Timer\n",
    "start_time, st_exec, st_cpu = dt.datetime.now().replace(microsecond=0), time.time(), time.process_time()      \n",
    "        \n",
    "# For each date in the test horizon\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(dates))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(predict_quantiles)(\n",
    "        X = np.array(X_data),\n",
    "        y = np.array(Y_data),\n",
    "        date = date,\n",
    "        dates = ID_data['date'],\n",
    "        areas = ID_data['area'],\n",
    "        weightsModel = density_estimator,\n",
    "        quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995],\n",
    "        print_status = False) for date in dates)\n",
    "\n",
    "# Finalize\n",
    "results = pd.concat(results, keys=dates).reset_index(names=['date', '']).drop(columns='')\n",
    "\n",
    "# Time\n",
    "print('Time:', dt.datetime.now().replace(microsecond=0) - start_time)  \n",
    "print('>> Execution time:', np.around(time.time()-st_exec, 0), \"seconds\") \n",
    "print('>> CPU time:', np.around(time.process_time()-st_cpu, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f73790-f5b6-4aac-9c57-bf7aad077501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Pinball Loss\n",
    "def scaled_pinball_loss(p, q, q_saa, y, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "    \"\"\"\n",
    "    q = np.array(q).flatten()\n",
    "    q_saa = np.array(q_saa).flatten()\n",
    "    y = np.array(y).flatten()\n",
    "    \n",
    "\n",
    "    # Pinball Loss Model\n",
    "    pl = np.mean((y - q) * p * (q <= y) + (q - y) * (1 - p) * (q > y))\n",
    "\n",
    "    # Pinball Loss SAA\n",
    "    pl_saa = np.mean((y - q_saa) * p * (q_saa <= y) + (q_saa - y) * (1 - p) * (q_saa > y))\n",
    "\n",
    "    # Scaled Pinball Loss\n",
    "    with np.errstate(divide='ignore'):\n",
    "        spl = (pl == pl_saa) * 1.0 + (pl != pl_saa) * (pl / pl_saa)\n",
    "\n",
    "    return spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea7ef2-021d-4cc9-a99d-b6eeb0574a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdfa68-4a1b-4d59-b43f-b23ba83700ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = results.groupby(['area', 'prob']).apply(\n",
    "    \n",
    "    lambda df: pd.Series({'spl': scaled_pinball_loss(p=df.prob, q=df.q_hat, q_saa=df.q_hat_saa, y=df.y)\n",
    "    \n",
    "    })\n",
    "\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b4d39-71ab-422c-841a-4d26a3afb04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8e6f1-0451-4df0-9547-823de3c931c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5d513-9171-4d3e-911f-2559cbd0c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f3d15-30b8-4fd7-bffc-c1ecbba432aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cases per day by treatment\n",
    "plotData = spl.groupby(['area'])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "for area, d in plotData:\n",
    "    ax.plot(d['prob'], d['spl'], marker='', linestyle='-', ms=2, linewidth=2, label=area)\n",
    "plt.axhline(y = 1, color = 'grey', linestyle = '--', linewidth=1)\n",
    "ax.legend()\n",
    "#ax.get_xaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ea818-4023-4860-b0a3-c215ef579d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ba5e4-d1ea-4182-aef5-14e0cae36672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec644e-e6c8-45c3-a564-94a7acaf6696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
